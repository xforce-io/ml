# 全局配置
global:
  cache_dir: "data/cache/"  # 默认缓存目录

# 模型配置
model:
  vocab_size: 50257  # GPT-2 词表大小
  max_position_embeddings: 1024
  hidden_size: 768   # GPT-2 small 配置
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1
  attention_type: "mha"  # 可选: mha, gqa, mqa
  num_kv_heads: null

# 训练配置
training:
  train_batch_size: 16
  eval_batch_size: 16
  learning_rate: 3.0e-4
  num_epochs: 3
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip: 1.0

# 数据集配置
data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  max_length: 512
  block_size: 128
  stride: 64
  train_subset_ratio: 0.2  # 使用 20% 的训练数据

# 实验配置
experiment:
  output_dir: "outputs"
  seed: 42
  task_name: "mrpc"  # 用于 GLUE benchmark 评估

# Weights & Biases 配置
wandb:
  enabled: false  # 是否启用 wandb
  project: "gpt-attention-comparison"  # 项目名称
  name: null  # 实验名称，null 则自动生成 