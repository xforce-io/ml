# 全局配置
global:
  cache_dir: "data/cache/"  # 默认缓存目录

# 模型配置
model:
  vocab_size: 50257  # GPT-2 词表大小
  max_position_embeddings: 1024
  hidden_size: 768   # GPT-2 small 配置
  num_layers: 12
  num_heads: 12
  intermediate_size: 3072
  dropout: 0.1
  attention_type: "mha"  # 可选: mha, gqa, mqa
  num_kv_heads: null

# 训练配置
training:
  train_batch_size: 16
  eval_batch_size: 16
  learning_rate: 3.0e-4
  num_epochs: 3
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_clip: 1.0

# 数据集配置
data:
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"
  max_length: 512
  block_size: 128
  stride: 64
  train_subset_ratio: 1.0

wandb:
  enabled: false
  project: "gpt-attention-comparison"
  name: null

# 实验配置
experiment:
  output_dir: "outputs"
  seed: 42
  task_name: "sst2"  # 用于 GLUE benchmark 评估 
